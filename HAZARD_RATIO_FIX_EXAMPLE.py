"""
GI·∫¢I PH√ÅP FIX HAZARD RATIO = 0 V√Ä EXTREME VALUES
================================================

V·∫•n ƒë·ªÅ: X2, X3 lu√¥n c√≥ HR = 0, c√°c ch·ªâ s·ªë kh√°c c√≥ HR c·ª±c ƒëoan (5174)
Nguy√™n nh√¢n: Thi·∫øu chu·∫©n h√≥a d·ªØ li·ªáu + Numerical instability
Gi·∫£i ph√°p: Chu·∫©n h√≥a + Regularization + Better interpretation

Author: TRUC-HUYNH
Date: 2025-11-11
"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from lifelines import CoxPHFitter
from scipy.stats.mstats import winsorize
from typing import Dict, List, Any, Tuple


class ImprovedSurvivalAnalysis:
    """
    Survival Analysis v·ªõi x·ª≠ l√Ω proper cho Hazard Ratios
    """

    def __init__(self):
        self.cox_model = None
        self.scaler = None
        self.feature_names = [
            'X_1', 'X_2', 'X_3', 'X_4', 'X_5', 'X_6', 'X_7',
            'X_8', 'X_9', 'X_10', 'X_11', 'X_12', 'X_13', 'X_14'
        ]
        self.feature_name_mapping = {
            'X_1': 'Bi√™n l·ª£i nhu·∫≠n g·ªôp',
            'X_2': 'Bi√™n l·ª£i nhu·∫≠n tr∆∞·ªõc thu·∫ø',
            'X_3': 'ROA',
            'X_4': 'ROE',
            'X_5': 'H·ªá s·ªë n·ª£ tr√™n t√†i s·∫£n',
            'X_6': 'H·ªá s·ªë n·ª£ tr√™n VCSH',
            'X_7': 'Kh·∫£ nƒÉng thanh to√°n hi·ªán h√†nh',
            'X_8': 'Kh·∫£ nƒÉng thanh to√°n nhanh',
            'X_9': 'Kh·∫£ nƒÉng tr·∫£ l√£i',
            'X_10': 'Kh·∫£ nƒÉng tr·∫£ n·ª£ g·ªëc',
            'X_11': 'Kh·∫£ nƒÉng t·∫°o ti·ªÅn/VCSH',
            'X_12': 'V√≤ng quay h√†ng t·ªìn kho',
            'X_13': 'K·ª≥ thu ti·ªÅn b√¨nh qu√¢n',
            'X_14': 'Hi·ªáu su·∫•t s·ª≠ d·ª•ng t√†i s·∫£n'
        }

    # ============================================================
    # GI·∫¢I PH√ÅP 1: CHU·∫®N H√ìA D·ªÆ LI·ªÜU + X·ª¨ L√ù OUTLIERS
    # ============================================================

    def prepare_data_improved(
        self,
        df: pd.DataFrame,
        duration_col: str = 'months_to_default',
        event_col: str = 'event',
        handle_outliers: bool = True,
        winsorize_limits: Tuple[float, float] = (0.01, 0.01)
    ) -> Tuple[pd.DataFrame, np.ndarray, np.ndarray]:
        """
        Chu·∫©n b·ªã d·ªØ li·ªáu v·ªõi proper scaling v√† outlier handling

        Args:
            df: DataFrame ch·ª©a 14 ch·ªâ s·ªë + duration + event
            duration_col: T√™n c·ªôt th·ªùi gian
            event_col: T√™n c·ªôt event
            handle_outliers: C√≥ x·ª≠ l√Ω outliers kh√¥ng
            winsorize_limits: Gi·ªõi h·∫°n winsorization (lower, upper)

        Returns:
            X_scaled: Features ƒë√£ chu·∫©n h√≥a
            durations: Array th·ªùi gian
            events: Array events
        """
        # L·∫•y 14 ch·ªâ s·ªë t√†i ch√≠nh
        X = df[self.feature_names].copy()

        # X·ª≠ l√Ω missing values
        X = X.fillna(X.median())

        # ‚úÖ B∆Ø·ªöC 1: X·ª≠ l√Ω outliers (n·∫øu c·∫ßn)
        if handle_outliers:
            print("üîß ƒêang x·ª≠ l√Ω outliers b·∫±ng winsorization...")
            for col in X.columns:
                X[col] = winsorize(X[col], limits=winsorize_limits)
            print("‚úÖ ƒê√£ x·ª≠ l√Ω outliers")

        # ‚úÖ B∆Ø·ªöC 2: Chu·∫©n h√≥a d·ªØ li·ªáu (QUAN TR·ªåNG!)
        print("üîß ƒêang chu·∫©n h√≥a d·ªØ li·ªáu (StandardScaler)...")
        self.scaler = StandardScaler()
        X_scaled = pd.DataFrame(
            self.scaler.fit_transform(X),
            columns=X.columns,
            index=X.index
        )
        print("‚úÖ ƒê√£ chu·∫©n h√≥a d·ªØ li·ªáu")

        # In th·ªëng k√™ sau khi scale
        print("\nüìä Th·ªëng k√™ sau khi scale:")
        print(f"  Mean: {X_scaled.mean().mean():.6f} (should be ~0)")
        print(f"  Std: {X_scaled.std().mean():.6f} (should be ~1)")
        print(f"  Min: {X_scaled.min().min():.2f}")
        print(f"  Max: {X_scaled.max().max():.2f}")

        # L·∫•y duration v√† event
        durations = df[duration_col].values
        events = df[event_col].values if event_col in df.columns else np.ones(len(df))

        # ƒê·∫£m b·∫£o duration > 0
        durations = np.maximum(durations, 0.1)

        return X_scaled, durations, events

    # ============================================================
    # GI·∫¢I PH√ÅP 2: ELASTIC NET COX MODEL
    # ============================================================

    def train_cox_model_improved(
        self,
        df: pd.DataFrame,
        duration_col: str = 'months_to_default',
        event_col: str = 'event',
        penalizer: float = 0.1,
        l1_ratio: float = 0.5
    ) -> Dict[str, Any]:
        """
        Hu·∫•n luy·ªán Cox model v·ªõi Elastic Net regularization

        Args:
            df: Training data
            penalizer: Regularization strength (0.01-1.0)
            l1_ratio: 0=Ridge, 1=Lasso, 0.5=Elastic Net

        Returns:
            Dict ch·ª©a metrics
        """
        # Chu·∫©n b·ªã d·ªØ li·ªáu v·ªõi improved pipeline
        X_scaled, durations, events = self.prepare_data_improved(
            df, duration_col, event_col, handle_outliers=True
        )

        # T·∫°o DataFrame cho Cox model
        cox_data = X_scaled.copy()
        cox_data['duration'] = durations
        cox_data['event'] = events

        # ‚úÖ IMPROVED: Elastic Net Cox v·ªõi penalizer cao h∆°n
        print(f"\nüîß Training Cox model (penalizer={penalizer}, l1_ratio={l1_ratio})...")
        self.cox_model = CoxPHFitter(
            penalizer=penalizer,
            l1_ratio=l1_ratio  # Elastic Net
        )
        self.cox_model.fit(cox_data, duration_col='duration', event_col='event')

        # T√≠nh metrics
        c_index = self.cox_model.concordance_index_
        log_likelihood = self.cox_model.log_likelihood_

        print(f"‚úÖ Model trained successfully!")
        print(f"  C-index: {c_index:.4f}")
        print(f"  Log-likelihood: {log_likelihood:.2f}")

        # Ki·ªÉm tra coefficient range
        coef_min = self.cox_model.params_.min()
        coef_max = self.cox_model.params_.max()
        print(f"\nüìä Coefficient range:")
        print(f"  Min: {coef_min:.4f}")
        print(f"  Max: {coef_max:.4f}")

        if coef_max > 10 or coef_min < -10:
            print("‚ö†Ô∏è  WARNING: Coefficients still too extreme!")
            print("   Consider increasing penalizer or checking data quality")

        return {
            'model_type': 'Cox Proportional Hazards (Elastic Net)',
            'c_index': float(c_index),
            'log_likelihood': float(log_likelihood),
            'penalizer': penalizer,
            'l1_ratio': l1_ratio,
            'coef_range': [float(coef_min), float(coef_max)]
        }

    # ============================================================
    # GI·∫¢I PH√ÅP 3: IMPROVED HAZARD RATIO REPORTING
    # ============================================================

    def get_hazard_ratios_improved(
        self,
        top_k: int = 5,
        clip_hr: bool = True,
        hr_min: float = 0.001,
        hr_max: float = 1000.0
    ) -> List[Dict[str, Any]]:
        """
        L·∫•y hazard ratios v·ªõi proper handling c·ªßa extreme values

        Args:
            top_k: S·ªë l∆∞·ª£ng ch·ªâ s·ªë mu·ªën l·∫•y
            clip_hr: C√≥ clip HR v√†o kho·∫£ng [hr_min, hr_max] kh√¥ng
            hr_min: Gi√° tr·ªã HR t·ªëi thi·ªÉu
            hr_max: Gi√° tr·ªã HR t·ªëi ƒëa

        Returns:
            List c√°c dict v·ªõi feature info v√† HR
        """
        if self.cox_model is None:
            raise ValueError("Cox model not trained!")

        # L·∫•y coefficients v√† p-values
        coefficients = self.cox_model.params_
        p_values = self.cox_model.summary['p']
        confidence_intervals = self.cox_model.confidence_intervals_

        # T√≠nh hazard ratios
        hazard_ratios_raw = np.exp(coefficients)

        # ‚úÖ IMPROVED: Clip HR v√†o kho·∫£ng h·ª£p l√Ω (n·∫øu c·∫ßn)
        if clip_hr:
            hazard_ratios = np.clip(hazard_ratios_raw, hr_min, hr_max)
            print(f"\nüîß Clipping HR to [{hr_min}, {hr_max}]")
        else:
            hazard_ratios = hazard_ratios_raw

        # T·∫°o list k·∫øt qu·∫£
        results = []
        for feature in self.feature_names:
            if feature in hazard_ratios.index:
                coef = float(coefficients[feature])
                hr = float(hazard_ratios[feature])
                hr_raw = float(hazard_ratios_raw[feature])
                p_val = float(p_values[feature])

                # ‚úÖ IMPROVED: Th√™m nhi·ªÅu metrics h∆°n
                result = {
                    'feature_code': feature,
                    'feature_name': self.feature_name_mapping[feature],

                    # Coefficient (log HR)
                    'coefficient': coef,

                    # Hazard Ratios
                    'hazard_ratio': hr,
                    'hazard_ratio_raw': hr_raw,  # Gi√° tr·ªã g·ªëc tr∆∞·ªõc khi clip
                    'was_clipped': hr != hr_raw,  # C√≥ b·ªã clip kh√¥ng

                    # Statistical significance
                    'p_value': p_val,
                    'ci_lower': float(confidence_intervals.loc[feature].iloc[0]),
                    'ci_upper': float(confidence_intervals.loc[feature].iloc[1]),
                    'significance': 'C√≥ √Ω nghƒ©a (p<0.05)' if p_val < 0.05 else 'Kh√¥ng c√≥ √Ω nghƒ©a (p‚â•0.05)',
                    'is_significant': p_val < 0.05,

                    # ‚úÖ IMPROVED: Di·ªÖn gi·∫£i d·ªÖ hi·ªÉu h∆°n
                    'interpretation': self._interpret_hazard_ratio(coef, hr, p_val)
                }

                results.append(result)

        # S·∫Øp x·∫øp theo absolute coefficient (not HR!)
        # V√¨ sau khi scale, coefficient ƒë√°ng tin h∆°n
        results.sort(key=lambda x: abs(x['coefficient']), reverse=True)

        return results[:top_k]

    def _interpret_hazard_ratio(self, coef: float, hr: float, p_val: float) -> str:
        """
        Di·ªÖn gi·∫£i Hazard Ratio m·ªôt c√°ch d·ªÖ hi·ªÉu
        """
        # Ki·ªÉm tra √Ω nghƒ©a th·ªëng k√™
        if p_val >= 0.05:
            return "‚ö™ Kh√¥ng c√≥ b·∫±ng ch·ª©ng th·ªëng k√™ v·ªÅ ·∫£nh h∆∞·ªüng"

        # N·∫øu c√≥ √Ω nghƒ©a th·ªëng k√™
        if coef > 0:
            # TƒÉng r·ªßi ro
            if hr < 1.5:
                return f"üü° TƒÉng r·ªßi ro nh·∫π ({(hr-1)*100:.1f}%)"
            elif hr < 2.0:
                return f"üü† TƒÉng r·ªßi ro trung b√¨nh ({(hr-1)*100:.1f}%)"
            else:
                return f"üî¥ TƒÉng r·ªßi ro m·∫°nh ({(hr-1)*100:.1f}%)"
        else:
            # Gi·∫£m r·ªßi ro
            risk_reduction = (1 - hr) * 100
            if hr > 0.67:
                return f"üü¢ Gi·∫£m r·ªßi ro nh·∫π ({risk_reduction:.1f}%)"
            elif hr > 0.5:
                return f"üü¢ Gi·∫£m r·ªßi ro trung b√¨nh ({risk_reduction:.1f}%)"
            else:
                return f"üü¢ Gi·∫£m r·ªßi ro m·∫°nh ({risk_reduction:.1f}%)"

    # ============================================================
    # GI·∫¢I PH√ÅP 4: PRETTY PRINT K·∫æT QU·∫¢
    # ============================================================

    def print_hazard_ratios_table(self, top_k: int = 5):
        """
        In b·∫£ng Hazard Ratios ƒë·∫πp v√† d·ªÖ ƒë·ªçc
        """
        hrs = self.get_hazard_ratios_improved(top_k=top_k, clip_hr=True)

        print("\n" + "="*100)
        print("üìä B·∫¢NG HAZARD RATIOS - TOP Y·∫æU T·ªê R·ª¶I RO QUAN TR·ªåNG".center(100))
        print("="*100)

        print("\nüí° Gi·∫£i th√≠ch Hazard Ratio (HR):")
        print("  ‚Ä¢ Coefficient > 0 (HR > 1): Ch·ªâ s·ªë TƒÇNG nguy c∆° v·ª° n·ª£")
        print("  ‚Ä¢ Coefficient < 0 (HR < 1): Ch·ªâ s·ªë GI·∫¢M nguy c∆° v·ª° n·ª£")
        print("  ‚Ä¢ P-value < 0.05: C√≥ √Ω nghƒ©a th·ªëng k√™")

        print("\n" + "-"*100)
        print(f"{'#':<3} {'Ch·ªâ s·ªë':<40} {'Coef':<8} {'HR':<10} {'P-value':<10} {'Di·ªÖn gi·∫£i':<30}")
        print("-"*100)

        for i, hr in enumerate(hrs, 1):
            feature_name = hr['feature_name']
            coef = hr['coefficient']
            hr_val = hr['hazard_ratio']
            p_val = hr['p_value']
            interp = hr['interpretation']

            # Highlight n·∫øu b·ªã clip
            if hr['was_clipped']:
                feature_name += " ‚ö†Ô∏è"

            print(f"{i:<3} {feature_name:<40} {coef:>7.3f} {hr_val:>9.3f} {p_val:>9.4f} {interp:<30}")

        print("-"*100)
        print("\n‚ö†Ô∏è  L∆∞u √Ω:")
        print("  ‚Ä¢ N·∫øu c√≥ ‚ö†Ô∏è: HR b·ªã clip v√¨ gi√° tr·ªã qu√° c·ª±c ƒëoan (b√°o hi·ªáu v·∫•n ƒë·ªÅ v·ªõi data/model)")
        print("  ‚Ä¢ N·∫øu p-value > 0.05: K·∫øt qu·∫£ kh√¥ng ƒë√°ng tin c·∫≠y v·ªÅ m·∫∑t th·ªëng k√™")
        print("  ‚Ä¢ N√™n t·∫≠p trung v√†o c√°c ch·ªâ s·ªë c√≥ p-value < 0.05")
        print("="*100 + "\n")

    # ============================================================
    # GI·∫¢I PH√ÅP 5: SO S√ÅNH TR∆Ø·ªöC/SAU IMPROVEMENT
    # ============================================================

    def compare_old_vs_new(self, df: pd.DataFrame):
        """
        So s√°nh k·∫øt qu·∫£ c·ªßa c√°ch c≈© vs c√°ch m·ªõi
        """
        print("\n" + "="*100)
        print("üî¨ SO S√ÅNH PH∆Ø∆†NG PH√ÅP C≈® VS M·ªöI".center(100))
        print("="*100)

        # C≈®: Kh√¥ng scale, penalizer th·∫•p
        print("\n1Ô∏è‚É£  PH∆Ø∆†NG PH√ÅP C≈® (Kh√¥ng scale, penalizer=0.01):")
        print("-"*100)
        old_system = SurvivalAnalysisOld()
        old_system.train_cox_model_old(df)
        old_hrs = old_system.get_hazard_ratios_old(top_k=5)

        for i, hr in enumerate(old_hrs, 1):
            print(f"  {i}. {hr['feature_name']}: HR={hr['hazard_ratio']:.3f}, p={hr['p_value']:.4f}")

        # M·ªöI: Scale + Elastic Net
        print("\n2Ô∏è‚É£  PH∆Ø∆†NG PH√ÅP M·ªöI (Scale + Elastic Net, penalizer=0.1):")
        print("-"*100)
        self.train_cox_model_improved(df, penalizer=0.1, l1_ratio=0.5)
        new_hrs = self.get_hazard_ratios_improved(top_k=5, clip_hr=True)

        for i, hr in enumerate(new_hrs, 1):
            print(f"  {i}. {hr['feature_name']}: HR={hr['hazard_ratio']:.3f}, p={hr['p_value']:.4f}")

        print("\n‚úÖ C·∫£i thi·ªán:")
        print("  ‚Ä¢ HR trong kho·∫£ng h·ª£p l√Ω h∆°n (kh√¥ng c√≥ 0 hay 5174)")
        print("  ‚Ä¢ P-values ƒë√°ng tin h∆°n")
        print("  ‚Ä¢ Coefficients ·ªïn ƒë·ªãnh h∆°n")
        print("="*100 + "\n")


# ============================================================
# CLASS C≈® (ƒê·ªÇ SO S√ÅNH)
# ============================================================

class SurvivalAnalysisOld:
    """Class c≈© ƒë·ªÉ so s√°nh"""

    def __init__(self):
        self.cox_model = None
        self.feature_names = [
            'X_1', 'X_2', 'X_3', 'X_4', 'X_5', 'X_6', 'X_7',
            'X_8', 'X_9', 'X_10', 'X_11', 'X_12', 'X_13', 'X_14'
        ]
        self.feature_name_mapping = {
            'X_1': 'Bi√™n l·ª£i nhu·∫≠n g·ªôp',
            'X_2': 'Bi√™n l·ª£i nhu·∫≠n tr∆∞·ªõc thu·∫ø',
            'X_3': 'ROA',
            'X_4': 'ROE',
            'X_5': 'H·ªá s·ªë n·ª£ tr√™n t√†i s·∫£n',
            'X_6': 'H·ªá s·ªë n·ª£ tr√™n VCSH',
            'X_7': 'Kh·∫£ nƒÉng thanh to√°n hi·ªán h√†nh',
            'X_8': 'Kh·∫£ nƒÉng thanh to√°n nhanh',
            'X_9': 'Kh·∫£ nƒÉng tr·∫£ l√£i',
            'X_10': 'Kh·∫£ nƒÉng tr·∫£ n·ª£ g·ªëc',
            'X_11': 'Kh·∫£ nƒÉng t·∫°o ti·ªÅn/VCSH',
            'X_12': 'V√≤ng quay h√†ng t·ªìn kho',
            'X_13': 'K·ª≥ thu ti·ªÅn b√¨nh qu√¢n',
            'X_14': 'Hi·ªáu su·∫•t s·ª≠ d·ª•ng t√†i s·∫£n'
        }

    def train_cox_model_old(self, df: pd.DataFrame):
        """Ph∆∞∆°ng ph√°p c≈© - KH√îNG SCALE"""
        X = df[self.feature_names].copy()
        X = X.fillna(X.median())  # Ch·ªâ fill NA, KH√îNG SCALE

        cox_data = X.copy()
        cox_data['duration'] = np.maximum(df['months_to_default'].values, 0.1)
        cox_data['event'] = df['event'].values

        self.cox_model = CoxPHFitter(penalizer=0.01)  # Penalizer th·∫•p
        self.cox_model.fit(cox_data, duration_col='duration', event_col='event')

    def get_hazard_ratios_old(self, top_k: int = 5):
        """Ph∆∞∆°ng ph√°p c≈© - KH√îNG CLIP"""
        hazard_ratios = np.exp(self.cox_model.params_)
        p_values = self.cox_model.summary['p']

        results = []
        for feature in self.feature_names:
            if feature in hazard_ratios.index:
                results.append({
                    'feature_name': self.feature_name_mapping[feature],
                    'hazard_ratio': float(hazard_ratios[feature]),
                    'p_value': float(p_values[feature])
                })

        results.sort(key=lambda x: abs(np.log(x['hazard_ratio'] + 1e-10)), reverse=True)
        return results[:top_k]


# ============================================================
# DEMO USAGE
# ============================================================

if __name__ == "__main__":
    print("\nüöÄ DEMO: FIX HAZARD RATIO = 0 PROBLEM")
    print("="*100)

    # T·∫°o sample data
    np.random.seed(42)
    n_samples = 200

    # T·∫°o 14 ch·ªâ s·ªë v·ªõi scale kh√°c nhau
    data = {
        'X_1': np.random.uniform(0.1, 0.4, n_samples),  # Bi√™n l·ª£i nhu·∫≠n g·ªôp
        'X_2': np.random.uniform(-0.1, 0.2, n_samples),  # Bi√™n LN tr∆∞·ªõc thu·∫ø (SCALE NH·ªé)
        'X_3': np.random.uniform(-0.05, 0.15, n_samples),  # ROA (SCALE NH·ªé)
        'X_4': np.random.uniform(-0.2, 0.3, n_samples),  # ROE
        'X_5': np.random.uniform(0.2, 0.9, n_samples),  # N·ª£/T√†i s·∫£n
        'X_6': np.random.uniform(0.5, 3.0, n_samples),  # N·ª£/VCSH
        'X_7': np.random.uniform(0.8, 2.5, n_samples),  # Thanh to√°n hi·ªán h√†nh
        'X_8': np.random.uniform(0.5, 2.0, n_samples),  # Thanh to√°n nhanh
        'X_9': np.random.uniform(1.0, 10.0, n_samples),  # Tr·∫£ l√£i
        'X_10': np.random.uniform(0.5, 5.0, n_samples),  # Tr·∫£ n·ª£ g·ªëc
        'X_11': np.random.uniform(-2.0, 2.0, n_samples),  # T·∫°o ti·ªÅn/VCSH
        'X_12': np.random.uniform(2.0, 15.0, n_samples),  # V√≤ng quay HTK
        'X_13': np.random.uniform(30, 90, n_samples),  # K·ª≥ thu ti·ªÅn
        'X_14': np.random.uniform(0.5, 2.0, n_samples),  # Hi·ªáu su·∫•t t√†i s·∫£n
    }

    df = pd.DataFrame(data)

    # T·∫°o synthetic survival data
    # C√¥ng ty c√≥ ROA th·∫•p, n·ª£ cao ‚Üí v·ª° n·ª£ s·ªõm
    risk_score = (
        -5 * df['X_3'] +  # ROA th·∫•p ‚Üí r·ªßi ro cao
        2 * df['X_5'] +   # N·ª£ cao ‚Üí r·ªßi ro cao
        -1 * df['X_2']    # L·ª£i nhu·∫≠n th·∫•p ‚Üí r·ªßi ro cao
    )

    df['months_to_default'] = np.clip(
        np.random.exponential(scale=1.0/np.exp(risk_score)) * 60,
        0.1, 120
    )
    df['event'] = np.random.binomial(1, 0.3, n_samples)  # 30% default rate

    print(f"\nüìä Sample data created: {n_samples} companies")
    print(f"  Default rate: {df['event'].mean()*100:.1f}%")
    print(f"  Median survival time: {df['months_to_default'].median():.1f} months")

    # Test improved method
    print("\n" + "="*100)
    print("üß™ TESTING IMPROVED METHOD")
    print("="*100)

    system = ImprovedSurvivalAnalysis()
    system.train_cox_model_improved(df, penalizer=0.1, l1_ratio=0.5)
    system.print_hazard_ratios_table(top_k=5)

    print("\n‚úÖ Demo completed! Check results above.")
    print("\nüí° Key Takeaways:")
    print("  1. Always scale your features before Cox regression")
    print("  2. Use proper regularization (Elastic Net)")
    print("  3. Clip extreme HR values for interpretability")
    print("  4. Focus on p-values < 0.05 for reliable results")
    print("  5. Consider using RSF if Cox is still unstable")
    print("="*100 + "\n")
